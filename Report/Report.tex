\documentclass{article}
\usepackage{graphicx}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{multirow}
\usepackage{subcaption}
\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}

\usepackage{xcolor}
\newcommand{\highlight}[1]{\colorbox{yellow!50}{$\displaystyle#1$}}
\hyphenpenalty=700

\title{Evolutionary Computation}
\author{Y1481702}
\date{\today}
\setlength\parindent{0pt}

\begin{document}
%FONT MUST BE SIZE 12 ARIAL.
%Margin must be MIN 2cm

\begin{titlepage}
\maketitle
\tableofcontents
\end{titlepage}

% Submit Code, 5 marks
% Must be a 'good final solution' and run on a test set

%\section{Abstract} %Unmarked - is it necessary? YES this is supposed to mirror a research paper
\section{Abstract}

% 15 marks
\section{Introduction}
% Well written and clear
% Connects well with the literature and provides relevant references
% Demonstates synthesis of the literature
% Demonstates critical ability
% Makes the problem and challenges clear
% Highlights the approach that will be used and justifies it

\subsection{Background}
Biomimicry, the imitation of nature for solving human problems has produced many examples of world class design such as Velcro(R), self-cleaning paints and Shinkansen high-speed trains\cite{biomimicry}.  Since the term biomimicry was first coined in 1997 by J. Benyus, this design philosophy has started to inspire many new developments. As well as the aforementioned mimics of physical systems in nature, natural processes have also been imitated, such as in the circular economy in product design and swarm robotics in computer science.
\\\\
Evolutionary Computation (EC) is a machine learning technique which is directly inspired by biological evolution. This is clearly another example of an imitation of nature's processes. Life has been around on earth for over 3.5 billion years, this is a lot of research \& development time to produce useful solutions which can be related to many of the current problems in a wide range of fields. Through evolution, nature has already produced its own solutions to many of the hardest machine learning problems of today such as vision.
\\\\
Machine learning usually focuses on optimisation of a particular goal, or set of goals, in a similar way that human engineers aim to produce optimal designs. Benyus sums this up well: `Computers can generate random ideas much faster than most engineers. And computers, not yet able to feel embarassment or peer pressure, are not afraid to try off-the-wall ideas. Ideas are just ideas; the more the merrier.'\cite[pp.209]{biomimicry}

\subsection{DEAP}
\textbf{D}istributed \textbf{E}volutionary \textbf{A}lgorithms in \textbf{P}ython (DEAP) is an open source framework for Python\cite{deap}. The framework aims to provide tools for quickly producing custom evolutionary algorithms. It provides a large amount of in-built functionality for implementing commonly used evolutionary algorithms and allows for parts of these to be mixed and matched as well as intermixed with custom sections. %WHICH VERSION OF DEAP WAS USED?

\subsection{EC for Snake}
The task given is to create an evolutionary algorithm in Python using the DEAP framework in order to play the classic video game, Snake. This is not a new task and has been the subject of previous work including implementations using genetic programming\cite{snake_blog} and more recently genetic algorithms\cite{snake_paper}. Despite this there is still room for further work, due to ongoing research into evolutionary computing and significant challenges because of the nature of the game.
\\\\
There are a variety of available instances of Snake with an array of different rules. In this case, there is a square game board of 14x14 cells as shown in Figure. \ref{fig:game_board}. The snake will continously move forwards in the direction it is facing, which can be changed by the user at any time. The aim is to collect food that appears in a random cell on the board when the previous one is eaten. Each time the snake eats a piece of food, it grows and takes up an extra cell. Once the snake fills every cell it can no longer grow. Hence, the highest possible score (185) is equal to the number of cells (196) less the initial length of the snake (11).
\\\\
%difficulties around the game that might cause issues
%PROBLEMS WITH THE TASK (SNAKE) in particular
Implementing this task will involve facing all of the usual challenges in evolutionary computation such as maintaining diversity and reducing bloat. Snake also brings along new challenges. The random placement of the food on the board adds an element of stochasticity to the solution meaning algorithm will perform differently across multiple runs and care will need to be taken to ensure that it works in the general case. The solution will also need to balance the need for the snake to avoid crashing into walls or itself while still picking up the food before time runs out. Optimising both of these objectives at the same time might be difficult.
\\\\
%general overview of the problems that will be discussed in the next section
%challenges that might be faced in this task
%GENERAL TASKS WITH GENETIC PROGRAMMING
The task provides some sensing functions are given that tell the snake if there is a wall, itself or food directly ahead. Additional sensing functions may, and will, be implemented in order to bring out important features of the game state that will require action to be taken. The brief states that no additional movement functions can be implemented, which relates to the fact that the controls of the game are fixed. It should be noted that while the movement functions have not been altered, some of the gameplay code has been refactored to allow the new movement functions to take advantage of existing code.

\section{Methods} %[35 marks..!]
% Quality of the design of the algorithm (e.g. choice of representation, fitness
% assessment, other details) to produce a snake AI. Give full details of the algorithm
% choices. Your should provide details and good rationale for your choices.
\subsection{Technology}
%INSERT LAB PC SPECS HERE
All the described algorithms have been written in Python 2.7.12 and DEAP Version 1.2. They have been run using standard University of York Computer Science lab PCs running Ubuntu 16.04 and containing Quad-Core Intel Core i7-4770 and 16GB of RAM.

\subsection{Representation}
% Discussion of Genotype vs Phenotype mapping, or if traits are 1:1
\subsubsection{Available Options}
%LECTURE: 'Representation of the problem (encoding) can help to better understand the landscape of the problem.'
%Representation can significantly affect the size of the search space and thus the time needed to search it
%Compresses the model and thus it requires less storage space, this is good because..?
The representation links the original problem to the search space over which the algorithm runs. As such, the type of representation chosen can significantly affect the size of the search space and by extension, the time needed to search it. Having a solution that is too abstract may reduce some required detail from the problem and remove the ability to produce good solutions. %CITATION NEEDED
The aim will be to encode the problem in a way that the representation requires minimal storage space and can easily search the problem space for good individuals. 

\myparagraph{Genetic Algorithms (GA)}
GAs are perhaps the most commonly used form of evolutionary algorithm. They are generally used for optimisation problems but have been applied to a variant of the snake game before\cite{snake_paper}. In this case, the GA was used to optimise four parameters (smoothness, space, food and direction) which determined the snake's movement. Deciding on important parameters and how an individual would react to different values of them seems to be a significant challenge of this type of implementation. It would tie the algorithm more closely to the game and reduce its ability to generalise.

\myparagraph{Evolutionary Strategy (ES)}
%TODO discuss

\myparagraph{Genetic Programming (GP)}
The aim of GP is to evolve programs that can be used to solve a problem. Clearly this is immediately more relevant for our snake implementation, the aim would be to create a program that can be run to determine our next move given the current state. GP uses a tree representation for programs, where the internal nodes are functions and the leaf nodes of the tree are constants and variables. GP has previously been explored by Ehlis\cite{snake_blog} and produced some promising results. Different function sets have been explored as well as a technique called priming which re-runs the algorithm starting with the best individuals and a new population if no good results are found.

\myparagraph{Neuroevolution}
Neuroevolution is a hybrid technique between evolutionary computing and artificial neural networks (ANN). The weights and structure of the neural network are initialised and then developed using an evolutionary algorithm. Unfortunately as the brief requires that the DEAP Python library is used, neuroevolution is not an available option as DEAP does not yet support it. While DEAP's open data structures and transparency allow for virtually any algorithm to be implemented, it will be easier far to use the built in structures.

\subsubsection{Choice and Justification}
Genetic Programming will be used as it most closely fits the aims set out in the introduction above. It intuitively maps to the problem, and will be the easiest to implement as the brief already contains a starter function set, which may be expanded, and a terminal set. A previous implementation of GP for snake is available online\cite{snake_blog}, this has been re-written in DEAP and slightly adapted to allow it to work with the available controls (up, down, left, right). Originally, this code was run with a population of 10,000 across 500 generations. Due to time constraints, this amount of computing was not feasible and needed to be considerably cut down. The final results have been generated using a population of 1000 across 75 generations, however the data for a full run of the control has been provided for completeness (Fig. \ref{table:population_size}). While the highly successful results from this original experiment could not be full reproduced, this new implementation gives a good baseline for judging further improvements. Any changes proposed in this report will be compared against this baseline in order to produce meaningful results. In order to avoid anomalous results caused by randomness, all results given are the mean result over 30 code runs.%TODO determine reasonable tolerance for generations AND population

\subsubsection{Physical Environments}
%MOVE THIS SECTION TO EVALUATION? ---------->>>
%Textbook: An important category of GP applications arises in contexts where executing a given expression changes the environment.
% ... THIS IS THE CASE IN SNAKE? See textbook Chap 6.10
This application of GP fits into a specific category involving a physical enviroment, which in this case is simulated. The added difficulty comes from the fact that the execution of any terminal element may change the environment. Once this change has been made, the subsequent execution may produce a different effect in the environment. In this kind of application the internal EA mechanics remains the same, but fitness evalutation can be significantly more computationally expensive\cite[p. 110]{textbook} in order to get a fair representation. In this example, as with the control, the fitness of a particular program will need to be determined over a full game, instead of a single turn. This will be discussed further in Section \ref{fitness_evaluation}.
\myparagraph{Terminal Set}
The terminal set uses the movement functions which, as previously mentioned, may not be altered. These functions change the directions of the snake to up, down, left and right. However, an additional terminal has been added which makes no changes to the current movement of the snake. This terminal should allow the snake to reduce its size by removing the need to check its current direction in order to maintain its current course.

\myparagraph{Function Set}
%Which additional sensing functions (if any) have been implemented?
Previous work has shown that the function set can help to find a better solution, however we need to be careful of overparameterising the problem. %cite for this
This is a common trade-off in many forms of machine learning. In this case, experiments %TODO- RESULTS FOR THIS?
and program analysis %TODO- show PROGRAM TREE
showed that by replacing the snake's ability to sense its direction with a terminal that allows it to maintain its current course resulted in a significant score boost as well as a drop in the overall program size.% GRAPHS PLZ

%Does the number of functions (primitives) affect the algorithm's performance?
%CITATIONS?

\subsection{Population}
\subsubsection{Initialisation Procedure}
DEAP offers all of the three main initialisation methods used in genetic programming. The Full method initialises a program tree where every individual in the population is the same depth along every path. The grow method creates trees with nodes that terminate before the maximum height is reached. The final type of initialisation procedure, ramped half and half, creates half of the trees using the full method and half using the grow method. This is what will be used for this algorithm as it will provides the greatest diversity in initial population.

\subsubsection{Size}
\label{population_size}
Having the correct population size is crucial to success\cite{dynamic_population_size}. The size of the population needs to be large enough to explore a significant amount of the search space by containing a number of promising individuals. If this is not the case, a large amount of mutation will be required to find any good solutions creating a very unstable solution. Too large a population will significantly impact the algorithm's performance. As the population size tends towards the size of the search space, the algorithm becomes no better than a brute force solution, running unnecessarily slowly. It has also been mathematically proved that, for certain problems, a larger population is highly likely lead to a sub-optimal solution if the problem has an attraction basin near some local optimum\cite{unhelpful_large_populations}.
\\\\
%fixed size or not? <--- probably, but WHY?! "In almost all EA applications the population size is constant and does not change during the evolutionary search." - Textbook WHY WHY WHY WHY?!?!?!! - citation needed
%easier to encode in an array? <- not if size of individuals might grow?
In almost all EA applications the population size is constant\cite[p. 20]{textbook}
%TALK ABOUT DYNAMIC population sizes in Spinosa \cite{dynamic_population_size}
\\\\
The algorithm was tested with varying population sizes to show the affect of this on the runtime and the fitness of the final individual produced. The algorithm was run 30 times for each population size, the runtime and fitness values for this are shown in Table \ref{table:population_size} and by Figure \ref{fig:pop_convergence}. After each run, the most fit individual was used to play 500 independent game, these results are shown in Table \ref{table:population_size} and by Figure \ref{fig:meanscore_v_runtime}. The runtime values only include the time used for evolution, not for testing the final individual.
\\\\
It is worth noting that the control has a mutation value of 0, so it requires that the initial population contains a reasonable distribution of good individuals. Adding mutation would allow novel ideas to be introduced later on in the evolution. This is likely why larger populations perform significantly better with this control algorithm and will be explored later in Section \ref{mutation}. Another interesting feature in the data is the significant increase in standard deviation as the population size increases. This shows that while larger populations help to produce individuals that perform better overall, this performance is not consistently across multiple games. This issue will be important to address later.
\\\\
The optimal size for the population is the size where a good solution is often produced but the program is quick to converge\cite{optimal_population_size}. A population of 1,000 seems to give a solution that is reasonably quick to converge (approx. 50 generations, Fig. \ref{fig:pop_convergence}) but also produces a reasonable score across multiple games (around 6.97, \ref{table:population_size}). This size of population will mean that proposed changes to the algorithm in this paper can be tested in a reasonable amount of time even with the limited timespan for the project and available computing resources.

\subsubsection{Diversity}
Within this evolutionary computation task, each individual of the population corresponds to a point in the search space of all possible programs. Diversity corresponds to having these points reasonably spread out throughout this space. A diverse population is, by definition, covering more of this search space than one that is not. Ensuring a diverse population is important to allow the algorithm to explore several promising ... at once\cite{diversity_recommended}. Having a large enough population can contribute to having a good initial diversity, but diversity needs to also be maintained throughout the run of the algorithm to prevent premature convergence.
%Can this be related back to k-nearest vs least-squares? Other machine learning problems

\subsection{Evaluation}
\label{fitness_evaluation}
%Multiple objectives?
%keep snake alive
%get more food!
Implementing a good fitness evaluation function is essential to evolving a solution to this problem. Without this, there will be no way to assess individuals in order to determine which will survive and reproduce. There are two major objectives at play in the game, avoiding crashing the snake and getting to the food before the timer hits zero. These can be represented by the final size of the snake when the game ends, as if the snake crashes then the game ends and it is unable to get to the food and grow. Therefore, we can simply consider this a single-objective problem where the objective is produce the maximum length of snake before either the snake crashes or the timer hits zero. This significantly simplifies the problem with minimal impact.
\\\\
The snake's food spawns randomly around the board. This element of stochasticity makes evaluating the fitness of a particular solution more difficult. The fitness of a given algorithm will likely change between games and this has already shown by previous results, as mentioned in Section \ref{population_size}. In order to ensure a fairer representation of fitness, each solution may need to be evaluated across multiple games.

\myparagraph{Incentives}
Certain scenarios in the game can lead to difficulties in evolving an individual that gets past a certain score. Rewarding individuals for making progress towards this, even if the score is not actually increased, can help to selectively breed programs that get past these boundaries. Likewise, punishing individuals that make obvious mistakes can also help to ensure they do not pass on these traits.
\\\\
A number of incentives and penalties have been employed to ensure maximum fitness:
\begin{itemize}
	\item Evaluating the fitness with multiple games, as described above will incentivise the snake to perform well generally rather than in only one particular fluke case.
	\item Adding a significant penalty to the score for crashing into a wall will ensure individuals never take this choice.
\end{itemize}
%TODO DETAIL.. how many games, how can we make this fair?
%Is each point worth the same?
	%do we want to incentivise consistently good scores or flukes?
A number of different fitness functions were tested.
Each of these have different benefits
- factorial (single run), increasingly rewards higher scores
- factorial (multiple runs), heavily rewards individuals which have a consistent high score

\subsection{Parent Selection}
%How do we pick which individuals continue?
Selecting which individuals should have influence over the next generation is one of the most important factors for success in Evolutionary Computing. If only the best individuals are selected then diversity will be lost, the algorithm will get stuck in a local minima and be unable to find a good general solution. This is known as premature convergence. If too many of the less-promising solutions are selected then the algorithm may be unstable meaning good solutions will take too long to find and may be lost quickly. Striking a good balance between a diverse population and a stable algorithm is necessary for finding a good solution in a reasonable time while preventing premature convergence.
\\\\
In biological evolution, old generations die to free up resources for newer generations.
Disregarding solutions due to age ... %good or bad? diversity, other measures?

% Losing good individuals? Hall of Fame/Elitism
Parent Selection can be random, so (if this is the case) good solutions may well die out. Elitism can be used to ensure that the best solution isn't randomly removed from the population. If no better solution is ever found, this solution will want to be present in the final population once evolution is complete.
%Does this have a positive/negative effect

%Roulette Wheel vs Tournament

\subsubsection{Maintaining Diversity}
% Don't want to get stuck in a local optimum
% Why is this a problem?
%Textbook: 'Premature convergence is the well-known effect of losing population diversity too quickly and getting trapped in a local optimum.'
Diversity is difficult to quantity in an objective manner and, as such, no single measure for it exists. A range of different measures including the number of different fitness values, number of different phenotypes/genotypes or entropy may be used\cite{textbook}.

\myparagraph{Roulette Wheel}
%description
%Pros, cons

\myparagraph{Tournament}
\label{tournament}
%description
%pros, cons

\subsubsection{Bloat}% How we work to prevent this,
Bloat, the increase of the size of a genetic program without significant increase in fitness, is a difficult problem to overcome. Bloat is linked to, but different from, overfitting, a common problem in many forms of machine learning\cite{measuring_bloat}. The real aim here is to find the smallest possible program tree for a particular fitness.
\\\\
All forms of bloat control have been shown to work best when augmented with a form of tree depth limiting\cite{parsimony_pressure, bloat_comparison}. In this case, the program trees have been limited to a height of 8, a value decided using experimentation. Controlling bloat while maximising fitness turns the evolution into a multi-objective optimisation problem. The two most commonly used solutions to bloat are parsimony pressure and double tournaments. Parsimony pressure subtracts a value based on the size of the program tree from the individual's fitness. Traditional parsimony presure is parametric and therefore a model that specifies an exact trade-off between values of model and size is needed. This is difficult to produce as this value may not be known before runtime and may not be the same between different individuals\cite{parsimony_pressure}. 
%\cite{parsimony_pressure}
\\\\
Double tournaments can be used for parent selection as an extension of regular tournament selection described in Section \ref{tournament}.
It has been shown that the order of a double tournament has no significant bearing on the end result. [cite, practical?]%Several recent papers discuss this issue and propose methods for tackling it, parsimony pressure above and.. \cite{multi_objective_bloat}

% Ongoing work into this may be interesting to talk about!

\subsection{Variation Operators}
% Describe how appropriate values for crossover and mutation were chosen to evolve the best agents,
\subsubsection{Mutation}
\label{mutation}
Mutation is the operator which incorporates new ideas into the population. Without it, genes would only be shuffled around and no new solutions could be found. %why is this bad..
%relate this to the search space..?
A trade-off is required here, too much mutation will result in a highly unstable algorithm. Far too much mutation would %move this to calibration section perhaps?
Care needs to be taken when implementing mutation so as not to create biases. This is particularly the case with real-valued mutation.. %which is not being used in this project?
%How to prevent biases?

\subsubsection{Crossover}
%Crossover-
%IF GA, is this fixed point crossover??

%multipoint crossover, uniform crossover, no crossover?
%cxOnePoint, cxTwoPoint, cxUniform, cxPartiallyMatched

\subsubsection{Calibration}
Extensive experimentation (Fig. \ref{fig:calibration}) has shown that for this population size (1000), reasonably high values for both mutation and crossover tend to produce individuals that work best in the game. Particlularly good values appear to be produced when each value is 0.75, so this will be used.

%Certain number of generations? - consistent across runs
%Certain fitness level? - guarantees a good solution, cannot be guaranteed to terminate
Certain number of generations..

\section{Results} %Results.. [35 marks]
% Use your evolutionary algorithm to find as good a solution as possible. Evaluate your
% snake(s) and your algorithm and use appropriate statistical methods in your reporting.

%randomness can cause odd results! we need to run multiple times, 30 replicates should be enough to get a broad idea of the evolutionary trends
%large numbers of runs can produce results that are significant but not important

%In order to evaluate we need to compare it with a control
While the control algorithm was originally intended to have a population of 10,000 and run for 500 generations, in order to make comparisons fair...
In order to make these comparisons fair, both algorithms have been run with a population of 1000 across 75 generations.
%designing controls can be difficult, they may be subject to: placebo effect & observer effect
\myparagraph{Scoring}
%USE A NULL HYPOTHESIS!!
%Non-parametric tests DO NOT require a distribution to be normal and should be the default choice, rank sum test??
A Wilcoxon rank sum test has been used to compare the scoring distribution for the new implementation, $S_N$, with the original control, $S_C$. This test is used as there is insufficient evidence to prove whether or not either of the distributions $S_N$ or $S_C$ are normal.
\\\\
The null hypothesis, $H_0$, states that the new algorithm produces results that are comparable to the control.
\[H_0: S_N = S_C\]
Therefore, the alternative hypothesis is-
\[H_1: S_N > S_C\]

We assume that $H_0$ is true. If the probability that $H_0$ is true, $p(H_0) < 0.05$, then there is sufficient evidence to reject the null hypothesis.

\myparagraph{Runtime}
Significant savings were also made on the runtime by developing an algorithm that produced good results even with a small population and limited number of generations. These is sufficient evidence to prove that runtime has been significantly sped up using runtime distributions, $T_N$ and $T_C$ for the new implementation and control, respectively.
\[H_0: T_N = T_C\]
\[H_1: T_N > T_C\]
%SEE Discovering Statistics using R
%SEE paper: Lecture 4 reading, A Practical Guide for Using Statistical Tests to Assess Randomized Algorithms in Software Testing

%NB. Test performance on different size/shaped game boards?

\section{Conclusion} %[10 marks]
% Well written and clear
\subsection{Main Findings}
% Summarizes findings well and highlights key points
% Connects well with the literature and provides relevant references
% Demonstrates critical ability
% Provides good suggestions for future work


%COMPARE to existing Blog and Paper snake solutions?

%Has DEAP proved a good tool for this task?
%pros/cons?

\subsection{Further Work}
The complexity and diversity of the snake game can lead into plenty of areas for further exploration. Recent developments in evolutionary computation such as co-evolution and neuroevolution could be explored in the context of this task. With co-evolution individuals would have to compete in order to get to the food items first, creating snake players that find the food faster.
\\\\
Alternative approaches to this problem are a promising avenue for future exploration. For example, a snake that cycles through every square on the board will always each piece of food as it traverses. An alternative implementation of this algorithm could have a function set with no information of the position of the food, but reward the snake using a fitness function based on the number of unique squares visited before returning to the start position. This alternative algorithm could be compared to the one described in this report to determine which approach is best.
\\\\
Future work could also explore the adaptability of this program to perform well across the wide number of variants of the game that exist with different formats and rules. This could be further expanded to attempt to create individuals that generalise across multiple game variants and carry their learning across in the way a human player would.

\newpage
\raggedright
\bibliography{Report}{}
\bibliographystyle{ieeetran}

\newpage
\section{Appendix}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|r|}
\hline%Add time taken?
\textbf{Population Size} & \multicolumn{2}{c|}{\textbf{Run Time}} & \multicolumn{2}{c|}{\textbf{Mean Test Run Score}} \\
\hline
\multirow{3}{*}{100}
& Mean & 6.59 & Mean & 2.76 \\
& Max & 13.5 & Max & 11.60 \\
& St.Dev & 2.52 & St.Dev & 2.14 \\
\hline
\multirow{3}{*}{250}
& Mean & 17.7 & Mean & 4.67 \\
& Max & 28.9 & Max & 18.8 \\
& St.Dev & 4.22 & St.Dev & 3.48 \\
\hline
\multirow{3}{*}{500}
& Mean & 43.39 & Mean & 5.81 \\
& Max & 121.75 & Max & 21.97 \\
& St.Dev & 18.55 & St.Dev & 3.99 \\
\hline
\multirow{3}{*}{750}
& Mean & 66.5 & Mean & 7.31 \\
& Max & 252.96 & Max & 23.9 \\
& St.Dev & 38.66 & St.Dev & 4.46 \\
\hline
\multirow{3}{*}{1000}
& Mean & 78.59 & Mean & 6.97 \\
& Max & 133.16 & Max & 24.20 \\
& St.Dev & 21.63 & St.Dev & 4.61 \\
\hline
\multirow{3}{*}{2,500}
& Mean & 249.24 & Mean & 10.73 \\
& Max & 518.06 & Max & 29.40 \\
& St.Dev & 83.79 & St.Dev & 5.73 \\
\hline
\multirow{3}{*}{5,000}
& Mean & 572.78 & Mean & 13.99 \\
& Max & 997.97 & Max & 33.83 \\
& St.Dev & 185.05 & St.Dev & 6.64 \\
\hline
\multirow{3}{*}{10000}
& Mean & 1437.17 & Mean & 18.0 \\
& Max & 39.27 & Max & 39.3 \\
& St.Dev & 7.59 & St.Dev & 7.59 \\
\hline
\end{tabular}
\end{center}
\caption{Population Size Performance over 250 Generations}
\label{table:population_size}
\end{table}

\begin{figure}[ht]
\centering
\begin{subfigure}{0.4\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{Figures/game_board_1}
\label{fig:game_board_1}
\end{subfigure}%
\begin{subfigure}{0.4\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{Figures/game_board_2}
\label{fig:game_board_2}
\end{subfigure}

\caption{The 14x14 Game Board}
\label{fig:game_board}
\end{figure}

\begin{figure}[ht]
\centering
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=1\textwidth]{Figures/convergence}
\caption{Mean Fitness across Generations}
\label{fig:pop_convergence}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=1\textwidth]{Figures/meanscore_v_runtime}
\caption{Mean Final Score and Run Time}
\label{fig:meanscore_v_runtime}
\end{subfigure}

\caption{Varying Population Size}
\label{fig:varying_population_size}
\end{figure}


\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{Figures/heatmap}

\caption{Varying Mutation and Crossover}
\label{fig:calibration}
\end{figure}


\begin{figure}[ht]
\centering
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=1\textwidth]{Figures/control_fitness}
\caption{Control}
\label{fig:control_fitness}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=1\textwidth]{Figures/final_fitness}
\caption{Final Algorithm}
\label{fig:final_fitness}
\end{subfigure}

\caption{Fitness of final algorithms.}
\label{fig:fitness_comparison}
\end{figure}

\end{document}
